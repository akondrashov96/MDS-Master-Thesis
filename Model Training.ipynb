{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение нейросетевых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом файле мы обучаем модели глубокого обучения для 2-х NER (Named Entity Recognition) задач:\n",
    "\n",
    " - Разбивку имени на составляющие (Фамилия, имя, отчество) - используем в качестве основы модель Bert-base\n",
    " - Разбивку адреса на составляющие (регион, район, город, ...) - используем в качестве основы модель rubert-base-cased\n",
    "\n",
    "Обе модели тонко настраиваются на специаально сгенерированных наборах данных. Генераторы наборов данных описаны в jupyter-ноутбуке Data Parser.\n",
    "\n",
    "А также загружаем модель M2M100 для исправления ошибок. Так как полноценное обучение и даже тонкая настройка данной модели представляет некоторую сложность, было принято решение провести квантизацию данной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoModelForTokenClassification, AutoTokenizer, \\\n",
    "    pipeline, DataCollatorForTokenClassification, get_scheduler\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исправление орфографии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Квантизация модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как базовая модель занимает достаточно много места (около 4.3 ГБ), мы можем провести квантизацию модели, чтобы уменьшить её размер на диске.  \n",
    "\n",
    "В целом, квантизация модели не должна приводить к значимой потери качества её предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"ai-forever/RuM2M100-1.2B\" \n",
    "\n",
    "path_to_model = \"model/M2M100ForConditionalGeneration/\" \n",
    "path_to_tokenizer = \"model/M2M100Tokenizer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_M100_spell = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer_M100_spell = M2M100Tokenizer.from_pretrained(path_to_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_M100_spell.save_pretrained(\"model/M2M100ForConditionalGeneration/\")\n",
    "# tokenizer_M100_spell.save_pretrained(\"model/M2M100Tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(model):\n",
    "\n",
    "    '''\n",
    "    Calculates size of the model\n",
    "    '''\n",
    "    \n",
    "    torch.save(model.state_dict(), \"./tmp/model.p\")\n",
    "    size=os.path.getsize(\"./tmp/model.p\")\n",
    "    os.remove('./tmp/model.p')\n",
    "    return \"{:.3f} KB\".format(size / 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4386893.908 KB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(model_M100_spell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(model_M100_spell, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1157517.419 KB'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_size(quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель достаточно сильно сжалась - примерно в 4 раза!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/M2M100_tokenizer/tokenizer_config.json',\n",
       " 'model/M2M100_tokenizer/special_tokens_map.json',\n",
       " 'model\\\\M2M100_tokenizer\\\\vocab.json',\n",
       " 'model\\\\M2M100_tokenizer\\\\sentencepiece.bpe.model',\n",
       " 'model/M2M100_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(quantized_model, \"model/M2M100_spellchecker/quantized_model.pt\")\n",
    "tokenizer_M100_spell.save_pretrained(\"model/M2M100_tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что квантизованная модель приемлемо работает на наборе данных из формы и нескольких произвольных примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:314: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(14341, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(14341, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): DynamicQuantizedLinear(in_features=1024, out_features=8192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (fc2): DynamicQuantizedLinear(in_features=8192, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(14341, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (q_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (out_proj): DynamicQuantizedLinear(in_features=1024, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): DynamicQuantizedLinear(in_features=1024, out_features=8192, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (fc2): DynamicQuantizedLinear(in_features=8192, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): DynamicQuantizedLinear(in_features=1024, out_features=14341, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_M100_spell = torch.load(\"model/M2M100_spellchecker/quantized_model.pt\")\n",
    "model_M100_spell.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tokenizer = \"model/M2M100_tokenizer/\"\n",
    "tokenizer_M100_spell = M2M100Tokenizer.from_pretrained(path_to_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_errors(sentence_in: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для исправления орфографии в модели \n",
    "\n",
    "    Параметры:\n",
    "    sentence_in : str\n",
    "        Предложение с (возможно) орфографическими ошибками\n",
    "\n",
    "    Возвращает:\n",
    "    answer : str\n",
    "        Предложение, очищенное от ошибок\n",
    "    \"\"\"\n",
    "\n",
    "    encodings = tokenizer_M100_spell(sentence_in, return_tensors=\"pt\")\n",
    "\n",
    "    generated_tokens = model_M100_spell.generate(**encodings, \n",
    "                                                 forced_bos_token_id=tokenizer_M100_spell.get_lang_id(\"ru\"), \n",
    "                                                 max_new_tokens = 200)\n",
    "    \n",
    "    answer = tokenizer_M100_spell.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    answer = re.sub('[a-zA-Z]+', '', answer[0]).strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Основая цель мероприятия 5 орпеля 2020 - практичиская оттработка навыкоф по ока занию помощь гражданов, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных проишествий, сокращение временной показатель реагирование.\n",
      "Output: ная цель мероприятия 5 апреля 2020 - практическая отработка навыков по оказанию помощь гражданам, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных происшествий, сокращение временной показатель реагирование.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Основая цель мероприятия 5 орпеля 2020 - практичиская оттработка навыкоф по ока занию помощь гражданов, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных проишествий, сокращение временной показатель реагирование.\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence_in=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Фомилию, имя, отчество не изменял\n",
      "Output: Фамилию, имя, отчество не изменял\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Фомилию, имя, отчество не изменял\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence_in=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Студент, Уфимский фелеал Масковского нифтеного института им. Академика И.М. Губкина\n",
      "Output: , Уфимский филиал Московского нефтяного института им. Академика И.М. Губкина\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Студент, Уфимский фелеал Масковского нифтеного института им. Академика И.М. Губкина\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence_in=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Имею загррничный паспорт 6543217 89, ОУМС пос. Хор Хабаровского края, 04.02.2012\n",
      "Output: Имею загрничный паспорт 654321789, ОУМС пос. Хор Хабаровского края, 04.02.2012\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Имею загррничный паспорт 6543217 89, ОУМС пос. Хор Хабаровского края, 04.02.2012\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence_in=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Калужская обл., г. Обнинск, ул. Гагарина, д. 5, кв.52\n",
      "Output: Калужская обл., г. Обнинск, ул. Гагарина, д. 5, кв.52\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Калужская обл., г. Обнинск, ул. Гагарина, д. 5, кв.52\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence_in=sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наблюдаются некоторые галлюционации у модели, однако в целом качество исправления опечаток приемлемо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общий раздел"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько общих функций, используемых при обучении NER моделей.   \n",
    "Примеры и процедура для кода взяты отсюда: https://huggingface.co/learn/nlp-course/chapter7/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids, label_names):\n",
    "\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        \n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        \n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            # if label % 2 == 1:\n",
    "            #     if label < len(label_names)-1:\n",
    "            #         label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, label_names, metric):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels, label_names):\n",
    "    \n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER (для имен)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве основной модели мы используем https://huggingface.co/ai-forever/ruBert-base. Эту модель также использовали при тонкой настройке модели viktoroo/sberbank-rubert-base-collection3, которая заточена на NER задачи. Для нашего проекта мы также будем делать тонкую настройку ruBert модели, но для конкретной задачи - вычленения из имени фамилии, имени и отчества.\n",
    "\n",
    "Примеры кода и процедура взяты отсюда: https://huggingface.co/learn/nlp-course/chapter7/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"viktoroo/sberbank-rubert-base-collection3\" \n",
    "# path_to_tokenizer = \"viktoroo/sberbank-rubert-base-collection3\"\n",
    "\n",
    "path_to_model = \"ai-forever/ruBert-base\" \n",
    "path_to_tokenizer = \"ai-forever/ruBert-base\"\n",
    "\n",
    "# path_to_model = \"DeepPavlov/rubert-base-cased\" \n",
    "# path_to_tokenizer = \"DeepPavlov/rubert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_NER = AutoTokenizer.from_pretrained(path_to_tokenizer, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае достаточно будет создавать только три сущности: имена, фамилии и отчества. Нулевую сущность мы можем опустить, так как в обрабатываемом документе в соответствующих ячейках всегда фигурируют только эти элементы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['PER-NAME', 'PER-SURN', 'PER-PATR']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer = tokenizer_NER):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids, label_names))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_NER = AutoModelForTokenClassification.from_pretrained(path_to_model,\n",
    "                                                               id2label=id2label,\n",
    "                                                               label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='ai-forever/ruBert-base', vocab_size=120138, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка модели (закомментируем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_NER.save_pretrained(\"model/model_NER_custom/\")\n",
    "# tokenizer_NER.save_pretrained(\"model/tokenizer_NER_custom/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для обычной модели\n",
    "\n",
    "# sentence = \"Меня зовут Вольфганг и я живу в Берлине\"\n",
    "\n",
    "# print(\"Input:\", sentence)\n",
    "\n",
    "# nlp = pipeline(\"ner\", model=model_NER, tokenizer=tokenizer_NER)\n",
    "\n",
    "# ner_results = nlp(sentence)\n",
    "\n",
    "# print(\"Output:\", ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join([elem['word'] for elem in ner_results[:2]]).replace('#', '').capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join([elem['word'] for elem in ner_results[2:]]).replace('#', '').capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем сгенерированные данные. Алгоритм генерации описан в блокноте \"Data parser.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2)\n"
     ]
    }
   ],
   "source": [
    "names_dataset = pd.read_json(\"data/names/names_train_large.json\")\n",
    "print(names_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим набор данных на обучающую, валидационную и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset['mode'] = np.random.choice(['train', \"val\", \"test\"], size = names_dataset.shape[0], p = [0.9, 0.05, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Олимпович, Аввакум, Копылов, Калашников]</td>\n",
       "      <td>[2, 0, 1, 1]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Горюнов, Арсентий, Соболев, Астафьев, Пахомович]</td>\n",
       "      <td>[1, 0, 1, 1, 2]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Фотиевна, Гертруда, Чижова]</td>\n",
       "      <td>[2, 0, 1]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Троицкий, Юзеф, Бенедиктович]</td>\n",
       "      <td>[1, 0, 2]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Людмила, Фадеева, Прохоровна, Грибкова]</td>\n",
       "      <td>[0, 1, 2, 1]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens         ner_tags   mode\n",
       "0          [Олимпович, Аввакум, Копылов, Калашников]     [2, 0, 1, 1]  train\n",
       "1  [Горюнов, Арсентий, Соболев, Астафьев, Пахомович]  [1, 0, 1, 1, 2]    val\n",
       "2                       [Фотиевна, Гертруда, Чижова]        [2, 0, 1]  train\n",
       "3                     [Троицкий, Юзеф, Бенедиктович]        [1, 0, 2]  train\n",
       "4           [Людмила, Фадеева, Прохоровна, Грибкова]     [0, 1, 2, 1]  train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset['ner_tags'] = names_dataset['ner_tags'].map(lambda x: [i-1 for i in x])\n",
    "names_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 36015\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 2027\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 1958\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"train\"])\n",
    "vldf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"val\"])\n",
    "tedf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"test\"])\n",
    "\n",
    "dataset_names = DatasetDict({\"train\": trdf, \"validation\": vldf, \"test\": tedf})\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd5c6e7bda54cddabe9015da06a364d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b61f94c8efc451c9341ffc34642075d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2027 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d60103bfc2947f7907949ba99c64f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1958 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_names.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_names[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    2,    2,    2,    0,    0,    0,    1,    1,    1,    1, -100],\n",
       "        [-100,    2,    2,    2,    0,    0,    1,    1, -100, -100, -100, -100],\n",
       "        [-100,    1,    1,    1,    0,    0,    2,    2, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    1,    1,    2,    2,    2,    1,    1, -100, -100]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(4)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 2, 2, 2, 0, 0, 0, 1, 1, 1, 1, -100]\n",
      "[-100, 2, 2, 2, 0, 0, 1, 1, -100]\n",
      "[-100, 1, 1, 1, 0, 0, 2, 2, -100]\n",
      "[-100, 0, 0, 1, 1, 2, 2, 2, 1, 1, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 36015\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 2027\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 1958\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, правильно ли встают коды сущностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-PATR', 'PER-NAME', 'PER-SURN', 'PER-SURN']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-PATR', 'PER-NAME', 'PER-SURN']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][1][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-SURN', 'PER-NAME', 'PER-PATR']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][2][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-SURN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-PATR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NAME': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PATR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'SURN': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.6666666666666666,\n",
       " 'overall_f1': 0.8,\n",
       " 'overall_accuracy': 0.6666666666666666}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NER.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_NER.parameters(), lr=5e-05, betas=(0.9,0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator(cpu=True)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model_NER, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "cos_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.1,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"model/bert-finetuned-ner-names-accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        cos_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered, label_names)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer_NER.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model_NER_names = \"model/bert-finetuned-ner-names-accelerate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NER для имен\n",
    "label_names = ['PER-NAME', 'PER-SURN', 'PER-PATR']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model_NER = AutoModelForTokenClassification.from_pretrained(path_to_model_NER_names,\n",
    "                                                               id2label=id2label,\n",
    "                                                               label2id=label2id)\n",
    "tokenizer_NER = AutoTokenizer.from_pretrained(path_to_model_NER_names, use_fast=True)\n",
    "\n",
    "model_NER.cpu()\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_NER, aggregation_strategy=\"simple\", tokenizer=tokenizer_NER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SURN', 'score': 0.9999862, 'word': 'мака', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999906, 'word': '##ров', 'start': 4, 'end': 7}, {'entity_group': 'NAME', 'score': 0.9999775, 'word': 'андреи', 'start': 8, 'end': 14}, {'entity_group': 'PATR', 'score': 0.99999034, 'word': 'николаевич', 'start': 15, 'end': 25}]\n"
     ]
    }
   ],
   "source": [
    "print(token_classifier(\"Макаров Андрей Николаевич\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SURN', 'score': 0.9999864, 'word': 'окро', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999907, 'word': '##шковская', 'start': 4, 'end': 12}, {'entity_group': 'NAME', 'score': 0.9999781, 'word': 'елена', 'start': 13, 'end': 18}, {'entity_group': 'PATR', 'score': 0.99999046, 'word': 'викторовна', 'start': 19, 'end': 29}]\n",
      "[{'entity_group': 'SURN', 'score': 0.9999862, 'word': 'коба', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999906, 'word': '##ева', 'start': 4, 'end': 7}, {'entity_group': 'NAME', 'score': 0.9999781, 'word': 'виола', 'start': 8, 'end': 13}, {'entity_group': 'PATR', 'score': 0.99999046, 'word': 'ролландовна', 'start': 14, 'end': 25}]\n",
      "[{'entity_group': 'SURN', 'score': 0.9999865, 'word': 'мака', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999907, 'word': '##рова', 'start': 4, 'end': 8}, {'entity_group': 'SURN', 'score': 0.8506638, 'word': '( пурпур', 'start': 9, 'end': 16}, {'entity_group': 'PATR', 'score': 0.9999869, 'word': '##ова )', 'start': 16, 'end': 20}, {'entity_group': 'NAME', 'score': 0.99997884, 'word': 'валентина', 'start': 21, 'end': 30}, {'entity_group': 'PATR', 'score': 0.99999034, 'word': 'михаиловна', 'start': 31, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "print(token_classifier(\"Окрошковская Елена Викторовна\"))\n",
    "\n",
    "print(token_classifier(\"Кобаева Виола Ролландовна\"))\n",
    "\n",
    "print(token_classifier(\"Макарова (Пурпурова) Валентина Михайловна\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_reconstruct(name: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для исправления формата имен в формат ФИО \n",
    "    В случае, если в тексте распознается более 1 фамилии, то используется формат \n",
    "        Ф (Ф1, Ф2, ... - при наличии старых фамилий) И О\n",
    "\n",
    "    Параметры:\n",
    "    name : str\n",
    "        Строка с именем\n",
    "\n",
    "    Возвращает:\n",
    "    string_out : str\n",
    "        Строка с именем требуемого формата\n",
    "    \"\"\"\n",
    "\n",
    "    # создание словаря для сортировки элементов имени\n",
    "    entities = ['SURN', 'NAME', 'PATR']\n",
    "    sort_dict = {key: elem for elem, key in list(enumerate(entities))}\n",
    "\n",
    "    name_tokens = re.findall(\"[а-яА-ЯЁё\\-]+\", name)\n",
    "\n",
    "    NER_output = token_classifier(name_tokens)\n",
    "\n",
    "    name_classes =  np.array([elem[0]['entity_group'] for elem in NER_output])\n",
    "\n",
    "    # переформирование имени\n",
    "    string_out = \" \".join([x for _, x in sorted(zip(name_classes, name_tokens), key = lambda pair: sort_dict[pair[0]])])\n",
    "\n",
    "    nameparts_counts = np.unique(name_classes, return_counts=True)\n",
    "\n",
    "    # В случае, если больше одной фамилии - фамилии, следующие после 1й заключить в скобки\n",
    "    if \"SURN\" in nameparts_counts[0] and nameparts_counts[1][-1] > 1:\n",
    "\n",
    "        surnames = string_out.split()[:nameparts_counts[1][-1]]\n",
    "        other_name_part = string_out.split()[nameparts_counts[1][-1]:]\n",
    "\n",
    "        string_out = surnames[0] + \" (\" + \", \".join(surnames[1:]) + \") \" + \" \".join(other_name_part)\n",
    "\n",
    "    return(string_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Семёнов-Тян-Шанский Пётр Петрович'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_reconstruct(\"Пётр Петрович Семёнов-Тян-Шанский\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пурпурова (Ольшанская) Валентина'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_reconstruct(\"Валентина (Пурпурова, Ольшанская)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все работает корректно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER (для адресов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, процедура для обучения NER для распознвания адресов идентична той, что мы провели для модели для распознавания имен. Однако, мы использовали другую модель - на этот раз модель, принимающую во внимание регистр символов. Мы предполагаем, что такая модель будет гораздо лучше распознвать элементы адреса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Калужская обл., г. Обнинск, ул. Аксёнова, д. 33, кв.21, прибыла из г. Воскресенск Московской обл. в 1990 г.\"\n",
    "\n",
    "# print(\"Input:\", sentence)\n",
    "\n",
    "# nlp = pipeline(\"ner\", model=model_NER, tokenizer=tokenizer_NER)\n",
    "\n",
    "# ner_results = nlp(sentence)\n",
    "\n",
    "# print(\"Output:\", ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [elem['word'].replace(\"#\", \"\") for elem in ner_results if elem['entity'] in ['B-LOC', \"I-LOC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"ai-forever/ruBert-large\" \n",
    "# path_to_tokenizer = \"ai-forever/ruBert-large\"\n",
    "\n",
    "path_to_model = \"DeepPavlov/rubert-base-cased\" \n",
    "path_to_tokenizer = \"DeepPavlov/rubert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_NER = AutoTokenizer.from_pretrained(path_to_tokenizer, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\"O\",\n",
    "               \"LOC-REG\", \n",
    "               \"LOC-DIST\", \n",
    "               \"LOC-SETL\", \n",
    "               \"LOC-CDIST\", \n",
    "               \"LOC-STRT\", \n",
    "               \"LOC-HOUS\", \n",
    "               \"LOC-FLAT\"]\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer = tokenizer_NER):\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        if i == 4: print(labels)\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids, label_names))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_NER_adr = AutoModelForTokenClassification.from_pretrained(path_to_model,\n",
    "                                                               id2label=id2label,\n",
    "                                                               label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем набор данных (алгоритм генерации описан в блокноте \"Data parser.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2)\n"
     ]
    }
   ],
   "source": [
    "adr_dataset = pd.read_json(\"data/addresses/addresses_train_med_extra.json\")\n",
    "print(adr_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Семиэтажное, здание, находилось, по, адресу:,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 4, 4, 1, 1, 2, 2, 3, 3, 0, 0, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Проживает, по, адресу:, Нагорское, Деревня, д...</td>\n",
       "      <td>[0, 0, 0, 4, 4, 6, 6, 7, 7, 5, 5, 2, 2, 1, 1]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Проживает, по, адресу:, Боханский, р-н, Тойси...</td>\n",
       "      <td>[0, 0, 0, 2, 2, 3, 3, 6, 6, 5, 5, 7, 7, 1, 1, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Выставочный, зал, находится, по, адресу, Чишм...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 2, 2, 1, 1, 3, 3, 4, 4]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Проживает, по, адресу:, Солтонский, р-н, Стер...</td>\n",
       "      <td>[0, 0, 0, 2, 2, 3, 3, 1, 1, 4, 4, 0, 0, 0, 0, ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Семиэтажное, здание, находилось, по, адресу:,...   \n",
       "1  [Проживает, по, адресу:, Нагорское, Деревня, д...   \n",
       "2  [Проживает, по, адресу:, Боханский, р-н, Тойси...   \n",
       "3  [Выставочный, зал, находится, по, адресу, Чишм...   \n",
       "4  [Проживает, по, адресу:, Солтонский, р-н, Стер...   \n",
       "\n",
       "                                            ner_tags   mode  \n",
       "0  [0, 0, 0, 0, 0, 4, 4, 1, 1, 2, 2, 3, 3, 0, 0, ...  train  \n",
       "1      [0, 0, 0, 4, 4, 6, 6, 7, 7, 5, 5, 2, 2, 1, 1]  train  \n",
       "2  [0, 0, 0, 2, 2, 3, 3, 6, 6, 5, 5, 7, 7, 1, 1, ...  train  \n",
       "3            [0, 0, 0, 0, 0, 2, 2, 1, 1, 3, 3, 4, 4]  train  \n",
       "4  [0, 0, 0, 2, 2, 3, 3, 1, 1, 4, 4, 0, 0, 0, 0, ...  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_dataset['mode'] = np.random.choice(['train', \"val\", \"test\"], size = adr_dataset.shape[0], p = [0.9, 0.05, 0.05])\n",
    "adr_dataset['ner_tags'] = adr_dataset['ner_tags'].map(lambda x: [i for i in x])\n",
    "adr_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 1072\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 64\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 64\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf = Dataset.from_pandas(adr_dataset[adr_dataset['mode'] == \"train\"])\n",
    "vldf = Dataset.from_pandas(adr_dataset[adr_dataset['mode'] == \"val\"])\n",
    "tedf = Dataset.from_pandas(adr_dataset[adr_dataset['mode'] == \"test\"])\n",
    "\n",
    "dataset_adr = DatasetDict({\"train\": trdf, \"validation\": vldf, \"test\": tedf})\n",
    "dataset_adr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccec91c504004744b54be3fa3f834ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1072 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 2, 2, 3, 3, 1, 1, 4, 4, 0, 0, 0, 0, 3, 3, 7, 7, 2, 2, 6, 6, 4, 4, 5, 5]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 4, 4, 3, 3, 7, 7, 6, 6, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcbbc784039482e9becbef2b21e3006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 7, 7, 4, 4, 6, 6, 5, 5, 3, 3, 1, 1, 2, 2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a8f59c2e1d4042816641e40a9c0b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/64 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 7, 7, 6, 6, 5, 5, 3, 3, 4, 4, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset_adr.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_adr[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 2, 2, 2, 2, 2, 3, 3, 1, 1, 4, 4, 4, 0, 0, 0, 0, 3, 3, 3, 3, 7, 7, 7, 2, 2, 2, 2, 2, 2, 6, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, -100]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets['train'][4]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    4,    4,    4,    1,\n",
       "            1,    1,    1,    1,    2,    2,    2,    2,    2,    3,    3,    3,\n",
       "            3,    3,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    4,    4,    4,    6,    6,    7,    7,\n",
       "            7,    5,    5,    2,    2,    2,    1,    1, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    2,    2,    2,    2,    2,    3,    3,\n",
       "            3,    3,    3,    3,    6,    6,    6,    5,    5,    7,    7,    7,\n",
       "            1,    1,    4,    4,    4,    4,    0,    0,    0,    0,    1,    1,\n",
       "            4,    4,    4,    3,    3,    3,    3,    3,    3,    3,    2,    2,\n",
       "            2,    2,    2,    2, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    0,    2,    2,    2,    1,    1,\n",
       "            3,    3,    3,    3,    4,    4, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    2,    2,    2,    2,    2,    3,    3,\n",
       "            1,    1,    4,    4,    4,    0,    0,    0,    0,    3,    3,    3,\n",
       "            3,    7,    7,    7,    2,    2,    2,    2,    2,    2,    6,    6,\n",
       "            6,    6,    6,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
       "            5,    5,    5,    5,    5, -100]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer_NER)\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'LOC-CDIST',\n",
       " 'LOC-CDIST',\n",
       " 'LOC-REG',\n",
       " 'LOC-REG',\n",
       " 'LOC-DIST',\n",
       " 'LOC-DIST',\n",
       " 'LOC-SETL',\n",
       " 'LOC-SETL',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_adr[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC-CDIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC-REG seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC-DIST seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: LOC-SETL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CDIST': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'DIST': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'REG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'SETL': {'precision': 0.5,\n",
       "  'recall': 1.0,\n",
       "  'f1': 0.6666666666666666,\n",
       "  'number': 1},\n",
       " 'overall_precision': 0.8,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 0.888888888888889,\n",
       " 'overall_accuracy': 0.9473684210526315}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"LOC-SETL\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_NER_adr.parameters(), lr=5e-05, betas=(0.9,0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator(cpu=True)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model_NER_adr, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "cos_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.1,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"model/test/bert-finetuned-ner-addresses-accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcb2f49f05b4cc49a0209b2be4b2bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 1: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 2: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 3: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 4: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 5: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 6: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 7: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 8: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n",
      "epoch 9: {'precision': 0.9903474903474904, 'recall': 0.9827586206896551, 'f1': 0.9865384615384616, 'accuracy': 0.9967453213995118}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "\n",
    "    # model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # print(i, batch['labels'])\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        cos_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered, label_names)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer_NER.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, как хорошо работает наша модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model_NER_addresses = \"model/test/bert-finetuned-ner-addresses-accelerate\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NER для адресов\n",
    "label_names = [\"O\",\n",
    "               \"LOC-REG\", \n",
    "               \"LOC-DIST\", \n",
    "               \"LOC-SETL\", \n",
    "               \"LOC-CDIST\", \n",
    "               \"LOC-STRT\", \n",
    "               \"LOC-HOUS\", \n",
    "               \"LOC-FLAT\"]\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model_NER_adr = AutoModelForTokenClassification.from_pretrained(path_to_model_NER_addresses,\n",
    "                                                               id2label=id2label,\n",
    "                                                               label2id=label2id)\n",
    "\n",
    "tokenizer_NER_adr = AutoTokenizer.from_pretrained(path_to_model_NER_addresses, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_base_model = \"viktoroo/sberbank-rubert-base-collection3\" \n",
    "# path_to_base_tokenizer = \"viktoroo/sberbank-rubert-base-collection3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_base_NER = AutoTokenizer.from_pretrained(path_to_base_tokenizer, use_fast=True)\n",
    "# model_base_NER = AutoModelForTokenClassification.from_pretrained(path_to_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Калужская обл., г. Обнинск, ул. Аксёнова, д. 33, кв.21, прибыла из г. Воскресенск Московской обл. в 1990 г.\"\n",
    "\n",
    "# print(\"Input:\", sentence)\n",
    "\n",
    "# nlp = pipeline(\"ner\", model=model_base_NER, tokenizer=tokenizer_base_NER)\n",
    "\n",
    "# ner_results = nlp(sentence)\n",
    "\n",
    "# [print(i) for i in ner_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NER_adr.cpu()\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_NER_adr, aggregation_strategy=\"simple\", tokenizer=tokenizer_NER_adr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'REG',\n",
       "  'score': 0.99929416,\n",
       "  'word': 'Калужская обл.,',\n",
       "  'start': 0,\n",
       "  'end': 15},\n",
       " {'entity_group': 'SETL',\n",
       "  'score': 0.9966365,\n",
       "  'word': 'г. Обнинск',\n",
       "  'start': 16,\n",
       "  'end': 26},\n",
       " {'entity_group': 'STRT',\n",
       "  'score': 0.8942354,\n",
       "  'word': ', ул. Университетская,',\n",
       "  'start': 26,\n",
       "  'end': 48},\n",
       " {'entity_group': 'HOUS',\n",
       "  'score': 0.9999078,\n",
       "  'word': 'д. 50',\n",
       "  'start': 49,\n",
       "  'end': 53}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier(\"Калужская обл., г. Обнинск, ул. Университетская, д.50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'REG', 'score': 0.9996962, 'word': 'Калужская обл.', 'start': 0, 'end': 14}]\n",
      "[{'entity_group': 'SETL', 'score': 0.9986812, 'word': 'г. Обнинск', 'start': 0, 'end': 10}]\n",
      "[{'entity_group': 'STRT', 'score': 0.99980533, 'word': 'ул. Университетская', 'start': 0, 'end': 19}]\n",
      "[{'entity_group': 'HOUS', 'score': 0.9998743, 'word': 'д. 50', 'start': 0, 'end': 4}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in token_classifier((\"Калужская обл., г. Обнинск, ул. Университетская, д.50\").split(\", \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'REG', 'score': 0.9919765, 'word': 'Республика Башкортостан,', 'start': 21, 'end': 45}\n",
      "{'entity_group': 'SETL', 'score': 0.89499885, 'word': 'г. Салават,', 'start': 46, 'end': 57}\n",
      "{'entity_group': 'STRT', 'score': 0.9946023, 'word': 'б - р Космонавтов,', 'start': 58, 'end': 74}\n",
      "{'entity_group': 'HOUS', 'score': 0.9998271, 'word': 'д. 14,', 'start': 75, 'end': 81}\n",
      "{'entity_group': 'FLAT', 'score': 0.9998968, 'word': 'кв. 67', 'start': 82, 'end': 88}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in token_classifier((\"Проживала по адресу: Республика Башкортостан, г. Салават, б-р Космонавтов, д. 14, кв. 67\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'REG', 'score': 0.99983895, 'word': 'Республика Башкортостан', 'start': 21, 'end': 44}]\n",
      "[{'entity_group': 'SETL', 'score': 0.98052216, 'word': 'г. Салават', 'start': 0, 'end': 10}]\n",
      "[{'entity_group': 'STRT', 'score': 0.9761292, 'word': 'б - р Космонавтов', 'start': 0, 'end': 15}]\n",
      "[{'entity_group': 'HOUS', 'score': 0.99987745, 'word': 'д. 14', 'start': 0, 'end': 5}]\n",
      "[{'entity_group': 'FLAT', 'score': 0.9998687, 'word': 'кв. 67', 'start': 0, 'end': 6}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in token_classifier((\"Проживала по адресу: Республика Башкортостан, г. Салават, б-р Космонавтов, д. 14, кв. 67\").split(\", \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'REG', 'score': 0.9996654, 'word': 'Башкирской АССР', 'start': 0, 'end': 15}]\n",
      "[{'entity_group': 'SETL', 'score': 0.81834745, 'word': 'г.', 'start': 0, 'end': 2}, {'entity_group': 'REG', 'score': 0.48711446, 'word': 'Уфа', 'start': 3, 'end': 6}]\n",
      "[{'entity_group': 'STRT', 'score': 0.99980766, 'word': 'ул. Космонавтов', 'start': 0, 'end': 15}]\n",
      "[{'entity_group': 'HOUS', 'score': 0.9997428, 'word': 'д. 1', 'start': 0, 'end': 4}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in token_classifier((\"Башкирской АССР, г. Уфа, ул. Космонавтов, д. 1\").split(\", \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'REG', 'score': 0.9996962, 'word': 'Калужская обл.', 'start': 0, 'end': 14}]\n",
      "[{'entity_group': 'SETL', 'score': 0.9986812, 'word': 'г. Обнинск', 'start': 0, 'end': 10}]\n",
      "[{'entity_group': 'STRT', 'score': 0.9997964, 'word': 'ул. Аксёнова', 'start': 0, 'end': 12}]\n",
      "[{'entity_group': 'HOUS', 'score': 0.99987406, 'word': 'д. 33', 'start': 0, 'end': 5}]\n",
      "[{'entity_group': 'FLAT', 'score': 0.99983984, 'word': 'кв. 21', 'start': 0, 'end': 5}]\n",
      "[{'entity_group': 'SETL', 'score': 0.9339662, 'word': 'г. Воскресенск', 'start': 11, 'end': 25}, {'entity_group': 'REG', 'score': 0.9108238, 'word': 'Московской обл.', 'start': 26, 'end': 41}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in \\\n",
    " token_classifier((\"Калужская обл., г. Обнинск, ул. Аксёнова, д. 33, кв.21, прибыла из г. Воскресенск Московской обл. в 1990 г\").split(\", \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SETL', 'score': 0.7186454, 'word': 'г. Москва', 'start': 0, 'end': 9}]\n",
      "[{'entity_group': 'STRT', 'score': 0.9993895, 'word': 'ул. Новочерёмушкинская', 'start': 0, 'end': 22}]\n",
      "[{'entity_group': 'HOUS', 'score': 0.99986935, 'word': 'д. 27', 'start': 0, 'end': 5}]\n",
      "[{'entity_group': 'FLAT', 'score': 0.99985933, 'word': 'кв. 154', 'start': 0, 'end': 7}]\n",
      "[{'entity_group': 'SETL', 'score': 0.972817, 'word': 'г. Обнинска', 'start': 10, 'end': 21}, {'entity_group': 'REG', 'score': 0.99963623, 'word': 'Калужской обл.', 'start': 22, 'end': 36}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in \\\n",
    " token_classifier((\"г. Москва, ул. Новочерёмушкинская, д. 27, кв. 154, прибыл из г. Обнинска Калужской обл. в 2020 г.\").split(\", \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_group': 'SETL', 'score': 0.5084862, 'word': 'г', 'start': 0, 'end': 1}\n",
      "{'entity_group': 'REG', 'score': 0.7918929, 'word': '. Москва', 'start': 1, 'end': 9}\n",
      "{'entity_group': 'STRT', 'score': 0.9964458, 'word': 'ул. Новочерёмушкинская,', 'start': 11, 'end': 34}\n",
      "{'entity_group': 'HOUS', 'score': 0.9641217, 'word': 'д. 27,', 'start': 35, 'end': 41}\n",
      "{'entity_group': 'FLAT', 'score': 0.9998932, 'word': 'кв. 154', 'start': 42, 'end': 49}\n",
      "{'entity_group': 'SETL', 'score': 0.99644387, 'word': 'г. Обнинска', 'start': 61, 'end': 72}\n",
      "{'entity_group': 'REG', 'score': 0.99984425, 'word': 'Калужской обл.', 'start': 73, 'end': 87}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i) for i in token_classifier((\"г. Москва, ул. Новочерёмушкинская, д. 27, кв. 154, прибыл из г. Обнинска Калужской обл. в 2020 г.\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address_reconstruct(address: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для исправления формата адреса в формат Регион, район, город/поселок, улица, дом, квартира \n",
    "\n",
    "    Параметры:\n",
    "    address : str\n",
    "        Строка, содержащая адрес\n",
    "\n",
    "    Возвращает:\n",
    "    string_out : str\n",
    "        Строка с адресом требуемого формата\n",
    "    \"\"\"\n",
    "\n",
    "    # создание словаря для сортировки элементов адреса\n",
    "    entities = [\"O\", \"REG\", \"DIST\", \"SETL\", \"CDIST\", \"STRT\", \"HOUS\", \"FLAT\"]\n",
    "    \n",
    "    sort_dict = {key: elem for elem, key in list(enumerate(entities))}\n",
    "\n",
    "    adr_tokens = address.strip().split(\", \")\n",
    "\n",
    "    # print(adr_tokens)\n",
    "\n",
    "    NER_output = list(token_classifier(adr_tokens))\n",
    "\n",
    "    addresses = [[]]\n",
    "    adr_entities = [[]]\n",
    "    i = 0\n",
    "\n",
    "    for token, elem in zip(adr_tokens, NER_output):\n",
    "\n",
    "        if len(elem) > 1:\n",
    "\n",
    "            if elem[0][\"start\"] > 0:\n",
    "\n",
    "                i += 1\n",
    "                addresses.append([token[:elem[0][\"start\"]]])\n",
    "                adr_entities.append([\"O\"])\n",
    "\n",
    "            for subelem in elem:\n",
    "\n",
    "                addresses[i].append(subelem[\"word\"])\n",
    "                adr_entities[i].append(subelem[\"entity_group\"])\n",
    "\n",
    "            if elem[-1][\"end\"] < len(token)-1:\n",
    "\n",
    "                addresses[i].append(token[elem[-1][\"end\"]:])\n",
    "                adr_entities[i].append(\"O\")\n",
    "\n",
    "        else:\n",
    "            addresses[i].append(token)\n",
    "            adr_entities[i].append(elem[0][\"entity_group\"])\n",
    "\n",
    "    print(addresses)\n",
    "    print(adr_entities)\n",
    "\n",
    "    final_list = []\n",
    "\n",
    "    for adr_lst, entity in zip(addresses, adr_entities):      \n",
    "\n",
    "        print(\"tokens:\", adr_lst)\n",
    "        print(\"entities:\", entity)    \n",
    "\n",
    "        if entity[0] == \"O\":\n",
    "                idx_start = 1\n",
    "                left = adr_lst[0]\n",
    "        else:\n",
    "                idx_start = 0\n",
    "                left = \"\"\n",
    "\n",
    "        if entity[-1] == \"O\":\n",
    "                idx_end = len(entity) - 1\n",
    "                right = adr_lst[-1]\n",
    "        else:\n",
    "                idx_end = len(entity)\n",
    "                right = \"\"\n",
    "\n",
    "        adr_sorted = [x for _, x in sorted(zip(entity[idx_start:idx_end], adr_lst[idx_start:idx_end]), \\\n",
    "                                            key = lambda pair: sort_dict[pair[0]])]\n",
    "\n",
    "        final_list.append(left + \", \".join(adr_sorted) + right)\n",
    "        \n",
    "        # print(final_list)\n",
    "\n",
    "    # # переформирование адреса\n",
    "    string_out = \", \".join(final_list)\n",
    "\n",
    "    return(string_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ул. Новочерёмушкинская', 'д. 27', 'кв. 154', 'г. Москва'], ['прибыл из ', 'г. Обнинска', 'Калужской обл.', ' в 2020 г.']]\n",
      "[['STRT', 'HOUS', 'FLAT', 'SETL'], ['O', 'SETL', 'REG', 'O']]\n",
      "tokens: ['ул. Новочерёмушкинская', 'д. 27', 'кв. 154', 'г. Москва']\n",
      "entities: ['STRT', 'HOUS', 'FLAT', 'SETL']\n",
      "tokens: ['прибыл из ', 'г. Обнинска', 'Калужской обл.', ' в 2020 г.']\n",
      "entities: ['O', 'SETL', 'REG', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'г. Москва, ул. Новочерёмушкинская, д. 27, кв. 154, прибыл из Калужской обл., г. Обнинска в 2020 г.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_text = \"ул. Новочерёмушкинская, д. 27, кв. 154, г. Москва, прибыл из г. Обнинска Калужской обл. в 2020 г.\"\n",
    "address_reconstruct(adr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['г. Москва', 'Старокалужское ш.', 'д. 62', 'корп. 4', 'стр. 1']]\n",
      "[['SETL', 'SETL', 'HOUS', 'HOUS', 'HOUS']]\n",
      "tokens: ['г. Москва', 'Старокалужское ш.', 'д. 62', 'корп. 4', 'стр. 1']\n",
      "entities: ['SETL', 'SETL', 'HOUS', 'HOUS', 'HOUS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'г. Москва, Старокалужское ш., д. 62, корп. 4, стр. 1'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_text = \"г. Москва, Старокалужское ш., д. 62, корп. 4, стр. 1\"\n",
    "address_reconstruct(adr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['г. Москва', 'ул. Ярцевская', 'д. 29', 'корп.2', 'кв. 147']]\n",
      "[['SETL', 'STRT', 'HOUS', 'HOUS', 'FLAT']]\n",
      "tokens: ['г. Москва', 'ул. Ярцевская', 'д. 29', 'корп.2', 'кв. 147']\n",
      "entities: ['SETL', 'STRT', 'HOUS', 'HOUS', 'FLAT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'г. Москва, ул. Ярцевская, д. 29, корп.2, кв. 147'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_text = \"г. Москва, ул. Ярцевская, д. 29, корп.2, кв. 147\"\n",
    "address_reconstruct(adr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ул. Университетская', 'д.50', 'г. Обнинск', 'Калужская обл.']]\n",
      "[['STRT', 'HOUS', 'SETL', 'REG']]\n",
      "tokens: ['ул. Университетская', 'д.50', 'г. Обнинск', 'Калужская обл.']\n",
      "entities: ['STRT', 'HOUS', 'SETL', 'REG']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Калужская обл., г. Обнинск, ул. Университетская, д.50'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_reconstruct(\"ул. Университетская, д.50, г. Обнинск, Калужская обл.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ул. Ярцевская', 'д. 29', 'корп.2', 'кв. 147', 'г. Москва']]\n",
      "[['STRT', 'HOUS', 'HOUS', 'FLAT', 'SETL']]\n",
      "tokens: ['ул. Ярцевская', 'д. 29', 'корп.2', 'кв. 147', 'г. Москва']\n",
      "entities: ['STRT', 'HOUS', 'HOUS', 'FLAT', 'SETL']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'г. Москва, ул. Ярцевская, д. 29, корп.2, кв. 147'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_reconstruct(\"ул. Ярцевская, д. 29, корп.2, кв. 147, г. Москва\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['г. Хабаровск', 'Хабаровский край', 'Центральный р-н', 'ул. Гоголя', 'д. 42']]\n",
      "[['SETL', 'REG', 'DIST', 'STRT', 'HOUS']]\n",
      "tokens: ['г. Хабаровск', 'Хабаровский край', 'Центральный р-н', 'ул. Гоголя', 'д. 42']\n",
      "entities: ['SETL', 'REG', 'DIST', 'STRT', 'HOUS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Хабаровский край, Центральный р-н, г. Хабаровск, ул. Гоголя, д. 42'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_reconstruct(\"г. Хабаровск, Хабаровский край, Центральный р-н, ул. Гоголя, д. 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, качество распознавания элементов адреса очень неплохое, однако местами встречаются ошибки, вроде распознавание районов города как районов региона. В целом такие ошибки проблематично выловить по двум причинам - в наборе данных элементы адреса специально перемешаны, поскольку целью использования модели является восстановление порядка адреса - сначала регион, потом район, населенный пункт, район населенного пункта, улица, дом, квартира. Пользователь при вводе данных в форму может менять порядок некоторых элементов.\n",
    "\n",
    "Решение - не использовать нейросеть, или же после получения вывода нейросети провести дополнительную обработку результата, однако подобное решение требует создания валидатора адреса, что в принципе убирает необходимость в нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
