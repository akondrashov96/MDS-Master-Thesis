{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение нейросетевых моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, AutoModelForTokenClassification, AutoTokenizer, \\\n",
    "    pipeline, DataCollatorForTokenClassification, get_scheduler\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исправление орфографии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"ai-forever/RuM2M100-1.2B\" \n",
    "\n",
    "path_to_model = \"model/M2M100ForConditionalGeneration/\" \n",
    "path_to_tokenizer = \"model/M2M100Tokenizer/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_M100_spell = M2M100ForConditionalGeneration.from_pretrained(path_to_model)\n",
    "tokenizer_M100_spell = M2M100Tokenizer.from_pretrained(path_to_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_M100_spell.save_pretrained(\"model/M2M100ForConditionalGeneration/\")\n",
    "# tokenizer_M100_spell.save_pretrained(\"model/M2M100Tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_errors(sentence: str) -> str:\n",
    "\n",
    "    encodings = tokenizer_M100_spell(sentence, return_tensors=\"pt\")\n",
    "    generated_tokens = model_M100_spell.generate(**encodings, \n",
    "                                                forced_bos_token_id=tokenizer_M100_spell.get_lang_id(\"ru\"), \n",
    "                                                max_new_tokens = 200)\n",
    "    answer = tokenizer_M100_spell.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Основая цель мероприятия 5 орпеля 2020 - практичиская оттработка навыкоф по ока занию помощь гражданов, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных проишествий, сокращение временной показатель реагирование.\n",
      "Output: Основная цель мероприятия 5 апреля 2020 - практическая отработка навыков по оказанию помощь гражданам, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных происшествий, сокращение временной показатель реагирование.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Основая цель мероприятия 5 орпеля 2020 - практичиская оттработка навыкоф по ока занию помощь гражданов, попавшим в ДТП, а также повышение и совершенствование уровня профессиональной подготовки сотрудников МЧС при проведении аварийно-спасательных работ по ликвидации последствий дорожно-транспортных проишествий, сокращение временной показатель реагирование.\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Фомилию, имя, отчество не изменял\n",
      "Output: Register фамилию, имя, отчество не изменял\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Фомилию, имя, отчество не изменял\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Студент, Уфимский фелеал Масковского нифтеного института им. Академика И.М. Губкина\n",
      "Output: Secret Студент, Уфимский филиал Московского нефтяного института им. Академика И.М. Губкина\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Студент, Уфимский фелеал Масковского нифтеного института им. Академика И.М. Губкина\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence=sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Имею загррничный паспорт 6543217 89, ОУМС пос. Хор Хабаровского края, 04.02.2012\n",
      "Output: Имею загрничный паспорт 654321789, ОУМС пос. Хор Хабаровского края, 04.02.2012\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Имею загррничный паспорт 6543217 89, ОУМС пос. Хор Хабаровского края, 04.02.2012\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "print(\"Output:\", correct_errors(sentence=sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (для имен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = \"viktoroo/sberbank-rubert-base-collection3\" \n",
    "# path_to_tokenizer = \"viktoroo/sberbank-rubert-base-collection3\"\n",
    "\n",
    "path_to_model = \"ai-forever/ruBert-base\" \n",
    "path_to_tokenizer = \"ai-forever/ruBert-base\"\n",
    "\n",
    "# path_to_model = \"DeepPavlov/rubert-base-cased\" \n",
    "# path_to_tokenizer = \"DeepPavlov/rubert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['PER-NAME', 'PER-SURN', 'PER-PATR']\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ai-forever/ruBert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_NER = AutoModelForTokenClassification.from_pretrained(path_to_model,\n",
    "                                                               id2label=id2label,\n",
    "                                                               label2id=label2id)\n",
    "tokenizer_NER = AutoTokenizer.from_pretrained(path_to_tokenizer, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(120138, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='ai-forever/ruBert-base', vocab_size=120138, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_NER.save_pretrained(\"model/model_NER_custom/\")\n",
    "# tokenizer_NER.save_pretrained(\"model/tokenizer_NER_custom/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для обычной модели\n",
    "\n",
    "# sentence = \"Меня зовут Вольфганг и я живу в Берлине\"\n",
    "\n",
    "# print(\"Input:\", sentence)\n",
    "\n",
    "# nlp = pipeline(\"ner\", model=model_NER, tokenizer=tokenizer_NER)\n",
    "\n",
    "# ner_results = nlp(sentence)\n",
    "\n",
    "# print(\"Output:\", ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join([elem['word'] for elem in ner_results[:2]]).replace('#', '').capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\".join([elem['word'] for elem in ner_results[2:]]).replace('#', '').capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2)\n"
     ]
    }
   ],
   "source": [
    "names_dataset = pd.read_json(\"data/names/names_train.json\")\n",
    "print(names_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_dataset['mode'] = np.random.choice(['train', \"val\", \"test\"], size = names_dataset.shape[0], p = [0.8, 0.15, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Колесова, Потапова, Корнелия, Аникина, Беляев...</td>\n",
       "      <td>[1, 1, 0, 1, 2, 1]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Остап]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Логинов, Витаутас, Савицкий, Калашников, Левк...</td>\n",
       "      <td>[1, 0, 1, 1, 2]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Горячева, Куприяновна, Алекса]</td>\n",
       "      <td>[1, 2, 0]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Фотий, Медведев, Быкович, Терентьев, Верещагин]</td>\n",
       "      <td>[0, 1, 2, 1, 1]</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens            ner_tags  \\\n",
       "0  [Колесова, Потапова, Корнелия, Аникина, Беляев...  [1, 1, 0, 1, 2, 1]   \n",
       "1                                            [Остап]                 [0]   \n",
       "2  [Логинов, Витаутас, Савицкий, Калашников, Левк...     [1, 0, 1, 1, 2]   \n",
       "3                    [Горячева, Куприяновна, Алекса]           [1, 2, 0]   \n",
       "4   [Фотий, Медведев, Быкович, Терентьев, Верещагин]     [0, 1, 2, 1, 1]   \n",
       "\n",
       "    mode  \n",
       "0    val  \n",
       "1  train  \n",
       "2  train  \n",
       "3    val  \n",
       "4    val  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_dataset['ner_tags'] = names_dataset['ner_tags'].map(lambda x: [i-1 for i in x])\n",
    "names_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 11966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 2254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 780\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trdf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"train\"])\n",
    "vldf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"val\"])\n",
    "tedf = Dataset.from_pandas(names_dataset[names_dataset['mode'] == \"test\"])\n",
    "\n",
    "dataset_names = DatasetDict({\"train\": trdf, \"validation\": vldf, \"test\": tedf})\n",
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_NER.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def tokenize_and_align_labels(examples, tokenizer = tokenizer_NER):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0857700a788481fb996cd2a6dad937d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11966 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14960bc217be4d4999a43a058cfea2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2254 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0485acbb46354bd8a13409174eb46f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/780 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = dataset_names.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_names[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer_NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 11966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 2254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 780\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100],\n",
       "        [-100,    1,    2,    0,    0,    0,    1,    2,    2,    1,    2,    2,\n",
       "            2,    2, -100],\n",
       "        [-100,    1,    2,    1,    2,    2,    2,    1,    2,    0,    0, -100,\n",
       "         -100, -100, -100],\n",
       "        [-100,    1,    0,    0,    2,    2,    2, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(4)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, -100]\n",
      "[-100, 1, 2, 0, 0, 0, 1, 2, 2, 1, 2, 2, 2, 2, -100]\n",
      "[-100, 1, 2, 1, 2, 2, 2, 1, 2, 0, 0, -100]\n",
      "[-100, 1, 0, 0, 2, 2, 2, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 11966\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 2254\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'mode', '__index_level_0__'],\n",
       "        num_rows: 780\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-NAME']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][0][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-SURN', 'PER-NAME', 'PER-SURN', 'PER-SURN', 'PER-PATR']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][1][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PER-SURN', 'PER-SURN', 'PER-PATR', 'PER-SURN', 'PER-NAME']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataset_names[\"train\"][2][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-SURN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-PATR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NAME': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PATR': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1},\n",
       " 'SURN': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 0.75,\n",
       " 'overall_f1': 0.8571428571428571,\n",
       " 'overall_accuracy': 0.8}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[2] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_NER.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], \n",
    "    collate_fn=data_collator, \n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model_NER.parameters(), lr=5e-05, betas=(0.9,0.999), eps=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator = Accelerator(cpu=True)\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model_NER, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"cosine\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.1,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    \n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"model/bert-finetuned-ner-names-accelerate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430622f029044633802511ebb94263d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-SURN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-PATR seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\Artem Kondrashov\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PER-NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.997057296174485, 'recall': 0.997057296174485, 'f1': 0.997057296174485, 'accuracy': 0.9985177393133786}\n",
      "epoch 1: {'precision': 0.9984420979747274, 'recall': 0.9986149584487535, 'f1': 0.9985285207305462, 'accuracy': 0.999043702782825}\n",
      "epoch 2: {'precision': 0.9996537995499394, 'recall': 0.9995672868887927, 'f1': 0.9996105413475269, 'accuracy': 0.9997131108348475}\n",
      "epoch 3: {'precision': 0.9995672494374243, 'recall': 0.9996537695836579, 'f1': 0.999610507638378, 'accuracy': 0.9997131108348475}\n",
      "epoch 4: {'precision': 0.9995672494374243, 'recall': 0.9996537695836579, 'f1': 0.999610507638378, 'accuracy': 0.9997131108348475}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    for batch in eval_dataloader:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer_NER.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model, aggregation_strategy=\"simple\", tokenizer=tokenizer_NER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SURN', 'score': 0.9999559, 'word': 'мака', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999753, 'word': '##ров', 'start': 4, 'end': 7}, {'entity_group': 'NAME', 'score': 0.9999467, 'word': 'андреи', 'start': 8, 'end': 14}, {'entity_group': 'PATR', 'score': 0.99996763, 'word': 'николаевич', 'start': 15, 'end': 25}]\n"
     ]
    }
   ],
   "source": [
    "print(token_classifier(\"Макаров Андрей Николаевич\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'SURN', 'score': 0.99995637, 'word': 'окро', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999767, 'word': '##шковская', 'start': 4, 'end': 12}, {'entity_group': 'NAME', 'score': 0.999951, 'word': 'елена', 'start': 13, 'end': 18}, {'entity_group': 'PATR', 'score': 0.9999747, 'word': 'викторовна', 'start': 19, 'end': 29}]\n",
      "[{'entity_group': 'SURN', 'score': 0.99995744, 'word': 'коба', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.99997675, 'word': '##ева', 'start': 4, 'end': 7}, {'entity_group': 'NAME', 'score': 0.99995106, 'word': 'виола', 'start': 8, 'end': 13}, {'entity_group': 'PATR', 'score': 0.9999754, 'word': 'ролландовна', 'start': 14, 'end': 25}]\n",
      "[{'entity_group': 'SURN', 'score': 0.9999559, 'word': 'мака', 'start': 0, 'end': 4}, {'entity_group': 'PATR', 'score': 0.9999777, 'word': '##рова', 'start': 4, 'end': 8}, {'entity_group': 'SURN', 'score': 0.9993266, 'word': '( пурпур', 'start': 9, 'end': 16}, {'entity_group': 'PATR', 'score': 0.85804, 'word': '##ова )', 'start': 16, 'end': 20}, {'entity_group': 'NAME', 'score': 0.9999414, 'word': 'валентина', 'start': 21, 'end': 30}, {'entity_group': 'PATR', 'score': 0.9999745, 'word': 'михаиловна', 'start': 31, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "print(token_classifier(\"Окрошковская Елена Викторовна\"))\n",
    "\n",
    "print(token_classifier(\"Кобаева Виола Ролландовна\"))\n",
    "\n",
    "print(token_classifier(\"Макарова (Пурпурова) Валентина Михайловна\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_reconstruct(name: str) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Функция для исправления формата имен в формат ФИО \n",
    "    В случае, если в тексте распознается более 1 фамилии, то используется формат \n",
    "        Ф (Ф1, Ф2, ... - при наличии старых фамилий) И О\n",
    "\n",
    "    Параметры:\n",
    "    name : str\n",
    "        Строка с именем\n",
    "\n",
    "    Возвращает:\n",
    "    string_out : str\n",
    "        Строка с именем требуемого формата\n",
    "    \"\"\"\n",
    "\n",
    "    # создание словаря для сортировки элементов имени\n",
    "    entities = ['SURN', 'NAME', 'PATR']\n",
    "    sort_dict = {key: elem for elem, key in list(enumerate(entities))}\n",
    "\n",
    "    name_tokens = re.findall(\"[а-яА-ЯЁё\\-]+\", name)\n",
    "\n",
    "    NER_output = token_classifier(name_tokens)\n",
    "\n",
    "    name_classes =  np.array([elem[0]['entity_group'] for elem in NER_output])\n",
    "\n",
    "    # переформирование имени\n",
    "    string_out = \" \".join([x for _, x in sorted(zip(name_classes, name_tokens), key = lambda pair: sort_dict[pair[0]])])\n",
    "\n",
    "    nameparts_counts = np.unique(name_classes, return_counts=True)\n",
    "\n",
    "    # В случае, если больше одной фамилии - фамилии, следующие после 1й заключить в скобки\n",
    "    if \"SURN\" in nameparts_counts[0] and nameparts_counts[1][-1] > 1:\n",
    "\n",
    "        surnames = string_out.split()[:nameparts_counts[1][-1]]\n",
    "        other_name_part = string_out.split()[nameparts_counts[1][-1]:]\n",
    "\n",
    "        string_out = surnames[0] + \" (\" + \", \".join(surnames[1:]) + \") \" + \" \".join(other_name_part)\n",
    "\n",
    "    return(string_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Семёнов-Тян-Шанский Пётр Петрович'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_reconstruct(\"Пётр Петрович Семёнов-Тян-Шанский\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Пурпурова (Ольшанская) Валентина'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_reconstruct(\"Валентина (Пурпурова, Ольшанская)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER (для адресов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Калужская обл., г. Обнинск, ул. Аксёнова, д. 33, кв.21, прибыла из г. Воскресенск Московской обл. в 1990 г.\"\n",
    "\n",
    "print(\"Input:\", sentence)\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model_NER, tokenizer=tokenizer_NER)\n",
    "\n",
    "ner_results = nlp(sentence)\n",
    "\n",
    "print(\"Output:\", ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[elem['word'].replace(\"#\", \"\") for elem in ner_results if elem['entity'] in ['B-LOC', \"I-LOC\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
